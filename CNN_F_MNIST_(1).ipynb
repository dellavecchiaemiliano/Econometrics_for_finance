{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dellavecchiaemiliano/Econometrics_for_finance/blob/main/CNN_F_MNIST_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "nlzUkqdUbF7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k43RPAmP5lmt"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "6R5ln2TA5rbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading data**\n",
        "\n",
        "First, I download the dataset fashion-MNIST which contains 60000 training images and 10000 test images of fashion and clothing items taken from 10 classes. Each image is a standardized 28x28 size in grayscale (784 total pixels). Then I transform the data into a tensor which is a multidimensional matrix."
      ],
      "metadata": {
        "id": "8XBbgFdfbWgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.FashionMNIST(\"data\", train=True, download=True, transform=ToTensor(),)\n",
        "test_set = datasets.FashionMNIST(\"data\", train=False, download=True, transform=ToTensor(),)"
      ],
      "metadata": {
        "id": "VJ5UAV0652FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below, I divide my dataset into intervals of 64 samples in order to make faster the iteration and to reduce the variance of the model."
      ],
      "metadata": {
        "id": "VD_eaqqTr-MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "yJgZNEZx6cLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building CNN**\n",
        "Finally, I design the structure of the CNN. The number of input is given by the quality of the images, in this case a matrix of 28x28, then I set two hidden layers with 512 nodes that return 10 outputs, each output represent a class. In order to make faster the algorithm I use the ReLu as activation function."
      ],
      "metadata": {
        "id": "9DFmF3MFsf8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_c9Q5xV7njc",
        "outputId": "21c6dee8-4fc2-4ca4-e3e3-267f8c76c9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**\n",
        "At first, I specify the type of cost function, the type of gradient and the learning rate to take into account. Then I create the function \"train\" which minimize the cost function by using the backpropagation tool.  "
      ],
      "metadata": {
        "id": "UR6Kt9abuZmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "YW828D0j70Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, cost_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        cost = cost_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            cost, current = cost.item(), batch * len(X)\n",
        "            print(f\"loss: {cost:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "dnqPyZ6X8_27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "Now, I set the model in .eval() mode. It no longer compute the gradient and it returns the accuracy and the average loss of the model on the test set."
      ],
      "metadata": {
        "id": "5-5I96STvrTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_cost, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_cost += cost_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_cost /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_cost:>8f} \\n\")"
      ],
      "metadata": {
        "id": "iRRPO_zy9DfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, cost_fn, optimizer)\n",
        "    test(test_dataloader, model, cost_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmKJW17h92IP",
        "outputId": "07ad5804-8c2c-4631-9740-7174dbb11fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.309425  [    0/60000]\n",
            "loss: 2.168519  [ 6400/60000]\n",
            "loss: 1.802842  [12800/60000]\n",
            "loss: 1.496882  [19200/60000]\n",
            "loss: 1.133817  [25600/60000]\n",
            "loss: 1.033809  [32000/60000]\n",
            "loss: 1.003939  [38400/60000]\n",
            "loss: 0.858092  [44800/60000]\n",
            "loss: 0.865884  [51200/60000]\n",
            "loss: 0.803831  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.786845 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.789316  [    0/60000]\n",
            "loss: 0.831269  [ 6400/60000]\n",
            "loss: 0.581631  [12800/60000]\n",
            "loss: 0.774856  [19200/60000]\n",
            "loss: 0.671326  [25600/60000]\n",
            "loss: 0.635321  [32000/60000]\n",
            "loss: 0.709007  [38400/60000]\n",
            "loss: 0.676740  [44800/60000]\n",
            "loss: 0.703037  [51200/60000]\n",
            "loss: 0.634546  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.630151 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.560955  [    0/60000]\n",
            "loss: 0.642537  [ 6400/60000]\n",
            "loss: 0.442320  [12800/60000]\n",
            "loss: 0.665561  [19200/60000]\n",
            "loss: 0.586767  [25600/60000]\n",
            "loss: 0.557302  [32000/60000]\n",
            "loss: 0.587200  [38400/60000]\n",
            "loss: 0.635138  [44800/60000]\n",
            "loss: 0.667662  [51200/60000]\n",
            "loss: 0.544542  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.566531 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.466256  [    0/60000]\n",
            "loss: 0.558747  [ 6400/60000]\n",
            "loss: 0.387023  [12800/60000]\n",
            "loss: 0.607994  [19200/60000]\n",
            "loss: 0.529629  [25600/60000]\n",
            "loss: 0.515640  [32000/60000]\n",
            "loss: 0.530788  [38400/60000]\n",
            "loss: 0.633300  [44800/60000]\n",
            "loss: 0.650002  [51200/60000]\n",
            "loss: 0.484362  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.534934 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.410035  [    0/60000]\n",
            "loss: 0.514895  [ 6400/60000]\n",
            "loss: 0.356988  [12800/60000]\n",
            "loss: 0.568220  [19200/60000]\n",
            "loss: 0.486096  [25600/60000]\n",
            "loss: 0.484865  [32000/60000]\n",
            "loss: 0.497845  [38400/60000]\n",
            "loss: 0.628361  [44800/60000]\n",
            "loss: 0.628654  [51200/60000]\n",
            "loss: 0.448777  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.514429 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.371234  [    0/60000]\n",
            "loss: 0.489224  [ 6400/60000]\n",
            "loss: 0.334788  [12800/60000]\n",
            "loss: 0.539947  [19200/60000]\n",
            "loss: 0.454747  [25600/60000]\n",
            "loss: 0.463984  [32000/60000]\n",
            "loss: 0.474350  [38400/60000]\n",
            "loss: 0.617602  [44800/60000]\n",
            "loss: 0.607729  [51200/60000]\n",
            "loss: 0.428822  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.499237 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.341091  [    0/60000]\n",
            "loss: 0.471247  [ 6400/60000]\n",
            "loss: 0.318368  [12800/60000]\n",
            "loss: 0.520364  [19200/60000]\n",
            "loss: 0.430653  [25600/60000]\n",
            "loss: 0.449235  [32000/60000]\n",
            "loss: 0.456388  [38400/60000]\n",
            "loss: 0.604604  [44800/60000]\n",
            "loss: 0.588979  [51200/60000]\n",
            "loss: 0.417411  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.486492 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.318491  [    0/60000]\n",
            "loss: 0.456957  [ 6400/60000]\n",
            "loss: 0.304318  [12800/60000]\n",
            "loss: 0.505272  [19200/60000]\n",
            "loss: 0.408958  [25600/60000]\n",
            "loss: 0.437336  [32000/60000]\n",
            "loss: 0.442065  [38400/60000]\n",
            "loss: 0.591270  [44800/60000]\n",
            "loss: 0.571380  [51200/60000]\n",
            "loss: 0.409621  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.476328 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.301969  [    0/60000]\n",
            "loss: 0.443762  [ 6400/60000]\n",
            "loss: 0.294319  [12800/60000]\n",
            "loss: 0.492605  [19200/60000]\n",
            "loss: 0.390696  [25600/60000]\n",
            "loss: 0.426202  [32000/60000]\n",
            "loss: 0.429470  [38400/60000]\n",
            "loss: 0.578544  [44800/60000]\n",
            "loss: 0.555665  [51200/60000]\n",
            "loss: 0.404669  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.466695 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.288923  [    0/60000]\n",
            "loss: 0.432361  [ 6400/60000]\n",
            "loss: 0.284739  [12800/60000]\n",
            "loss: 0.479430  [19200/60000]\n",
            "loss: 0.375373  [25600/60000]\n",
            "loss: 0.417408  [32000/60000]\n",
            "loss: 0.417028  [38400/60000]\n",
            "loss: 0.565870  [44800/60000]\n",
            "loss: 0.543419  [51200/60000]\n",
            "loss: 0.400405  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.457862 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.278087  [    0/60000]\n",
            "loss: 0.421573  [ 6400/60000]\n",
            "loss: 0.275709  [12800/60000]\n",
            "loss: 0.466838  [19200/60000]\n",
            "loss: 0.363560  [25600/60000]\n",
            "loss: 0.409934  [32000/60000]\n",
            "loss: 0.405473  [38400/60000]\n",
            "loss: 0.556037  [44800/60000]\n",
            "loss: 0.531132  [51200/60000]\n",
            "loss: 0.396725  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.448979 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.269675  [    0/60000]\n",
            "loss: 0.412588  [ 6400/60000]\n",
            "loss: 0.268916  [12800/60000]\n",
            "loss: 0.455613  [19200/60000]\n",
            "loss: 0.352899  [25600/60000]\n",
            "loss: 0.403244  [32000/60000]\n",
            "loss: 0.395011  [38400/60000]\n",
            "loss: 0.546148  [44800/60000]\n",
            "loss: 0.520077  [51200/60000]\n",
            "loss: 0.393921  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.441503 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.263133  [    0/60000]\n",
            "loss: 0.404870  [ 6400/60000]\n",
            "loss: 0.263160  [12800/60000]\n",
            "loss: 0.445420  [19200/60000]\n",
            "loss: 0.343040  [25600/60000]\n",
            "loss: 0.396458  [32000/60000]\n",
            "loss: 0.386553  [38400/60000]\n",
            "loss: 0.536674  [44800/60000]\n",
            "loss: 0.510618  [51200/60000]\n",
            "loss: 0.389975  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.434316 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.257059  [    0/60000]\n",
            "loss: 0.396660  [ 6400/60000]\n",
            "loss: 0.258380  [12800/60000]\n",
            "loss: 0.434767  [19200/60000]\n",
            "loss: 0.333495  [25600/60000]\n",
            "loss: 0.391119  [32000/60000]\n",
            "loss: 0.378084  [38400/60000]\n",
            "loss: 0.526732  [44800/60000]\n",
            "loss: 0.501954  [51200/60000]\n",
            "loss: 0.386426  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 0.427567 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.251764  [    0/60000]\n",
            "loss: 0.389569  [ 6400/60000]\n",
            "loss: 0.254128  [12800/60000]\n",
            "loss: 0.424924  [19200/60000]\n",
            "loss: 0.324553  [25600/60000]\n",
            "loss: 0.386506  [32000/60000]\n",
            "loss: 0.370454  [38400/60000]\n",
            "loss: 0.518064  [44800/60000]\n",
            "loss: 0.492518  [51200/60000]\n",
            "loss: 0.384116  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 0.421588 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.247179  [    0/60000]\n",
            "loss: 0.382474  [ 6400/60000]\n",
            "loss: 0.250067  [12800/60000]\n",
            "loss: 0.414563  [19200/60000]\n",
            "loss: 0.316336  [25600/60000]\n",
            "loss: 0.381854  [32000/60000]\n",
            "loss: 0.362318  [38400/60000]\n",
            "loss: 0.510360  [44800/60000]\n",
            "loss: 0.485234  [51200/60000]\n",
            "loss: 0.381484  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.2%, Avg loss: 0.416304 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.243694  [    0/60000]\n",
            "loss: 0.375621  [ 6400/60000]\n",
            "loss: 0.245990  [12800/60000]\n",
            "loss: 0.405603  [19200/60000]\n",
            "loss: 0.309833  [25600/60000]\n",
            "loss: 0.377505  [32000/60000]\n",
            "loss: 0.355901  [38400/60000]\n",
            "loss: 0.502552  [44800/60000]\n",
            "loss: 0.476539  [51200/60000]\n",
            "loss: 0.379636  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.411042 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.239927  [    0/60000]\n",
            "loss: 0.368967  [ 6400/60000]\n",
            "loss: 0.243287  [12800/60000]\n",
            "loss: 0.395891  [19200/60000]\n",
            "loss: 0.304098  [25600/60000]\n",
            "loss: 0.373734  [32000/60000]\n",
            "loss: 0.349101  [38400/60000]\n",
            "loss: 0.494738  [44800/60000]\n",
            "loss: 0.468641  [51200/60000]\n",
            "loss: 0.376906  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.406576 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.237204  [    0/60000]\n",
            "loss: 0.362270  [ 6400/60000]\n",
            "loss: 0.239291  [12800/60000]\n",
            "loss: 0.388431  [19200/60000]\n",
            "loss: 0.300656  [25600/60000]\n",
            "loss: 0.369198  [32000/60000]\n",
            "loss: 0.343333  [38400/60000]\n",
            "loss: 0.486586  [44800/60000]\n",
            "loss: 0.461920  [51200/60000]\n",
            "loss: 0.374674  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.6%, Avg loss: 0.402554 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.234065  [    0/60000]\n",
            "loss: 0.357706  [ 6400/60000]\n",
            "loss: 0.236258  [12800/60000]\n",
            "loss: 0.379838  [19200/60000]\n",
            "loss: 0.296285  [25600/60000]\n",
            "loss: 0.366081  [32000/60000]\n",
            "loss: 0.337562  [38400/60000]\n",
            "loss: 0.479453  [44800/60000]\n",
            "loss: 0.454204  [51200/60000]\n",
            "loss: 0.371826  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.398129 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.231419  [    0/60000]\n",
            "loss: 0.352740  [ 6400/60000]\n",
            "loss: 0.233278  [12800/60000]\n",
            "loss: 0.372738  [19200/60000]\n",
            "loss: 0.293286  [25600/60000]\n",
            "loss: 0.361647  [32000/60000]\n",
            "loss: 0.332514  [38400/60000]\n",
            "loss: 0.472526  [44800/60000]\n",
            "loss: 0.447977  [51200/60000]\n",
            "loss: 0.369440  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.8%, Avg loss: 0.395346 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.230610  [    0/60000]\n",
            "loss: 0.348319  [ 6400/60000]\n",
            "loss: 0.231668  [12800/60000]\n",
            "loss: 0.366504  [19200/60000]\n",
            "loss: 0.290527  [25600/60000]\n",
            "loss: 0.357142  [32000/60000]\n",
            "loss: 0.327756  [38400/60000]\n",
            "loss: 0.465175  [44800/60000]\n",
            "loss: 0.438504  [51200/60000]\n",
            "loss: 0.367775  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.9%, Avg loss: 0.392211 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.227325  [    0/60000]\n",
            "loss: 0.343560  [ 6400/60000]\n",
            "loss: 0.229649  [12800/60000]\n",
            "loss: 0.360395  [19200/60000]\n",
            "loss: 0.287150  [25600/60000]\n",
            "loss: 0.352383  [32000/60000]\n",
            "loss: 0.323523  [38400/60000]\n",
            "loss: 0.457703  [44800/60000]\n",
            "loss: 0.431663  [51200/60000]\n",
            "loss: 0.364438  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.1%, Avg loss: 0.388660 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.224726  [    0/60000]\n",
            "loss: 0.338209  [ 6400/60000]\n",
            "loss: 0.226380  [12800/60000]\n",
            "loss: 0.353642  [19200/60000]\n",
            "loss: 0.285142  [25600/60000]\n",
            "loss: 0.347891  [32000/60000]\n",
            "loss: 0.319300  [38400/60000]\n",
            "loss: 0.449998  [44800/60000]\n",
            "loss: 0.424593  [51200/60000]\n",
            "loss: 0.361263  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.384948 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.222316  [    0/60000]\n",
            "loss: 0.333860  [ 6400/60000]\n",
            "loss: 0.223382  [12800/60000]\n",
            "loss: 0.346530  [19200/60000]\n",
            "loss: 0.282923  [25600/60000]\n",
            "loss: 0.344512  [32000/60000]\n",
            "loss: 0.315609  [38400/60000]\n",
            "loss: 0.443091  [44800/60000]\n",
            "loss: 0.420442  [51200/60000]\n",
            "loss: 0.357723  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.382166 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.220052  [    0/60000]\n",
            "loss: 0.329510  [ 6400/60000]\n",
            "loss: 0.221300  [12800/60000]\n",
            "loss: 0.341590  [19200/60000]\n",
            "loss: 0.281549  [25600/60000]\n",
            "loss: 0.340782  [32000/60000]\n",
            "loss: 0.312574  [38400/60000]\n",
            "loss: 0.437952  [44800/60000]\n",
            "loss: 0.413070  [51200/60000]\n",
            "loss: 0.354223  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.379063 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.217788  [    0/60000]\n",
            "loss: 0.324852  [ 6400/60000]\n",
            "loss: 0.218097  [12800/60000]\n",
            "loss: 0.334651  [19200/60000]\n",
            "loss: 0.279372  [25600/60000]\n",
            "loss: 0.336716  [32000/60000]\n",
            "loss: 0.309602  [38400/60000]\n",
            "loss: 0.432366  [44800/60000]\n",
            "loss: 0.407616  [51200/60000]\n",
            "loss: 0.350738  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.377146 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.216258  [    0/60000]\n",
            "loss: 0.321082  [ 6400/60000]\n",
            "loss: 0.215567  [12800/60000]\n",
            "loss: 0.329165  [19200/60000]\n",
            "loss: 0.278018  [25600/60000]\n",
            "loss: 0.334527  [32000/60000]\n",
            "loss: 0.306359  [38400/60000]\n",
            "loss: 0.426003  [44800/60000]\n",
            "loss: 0.399855  [51200/60000]\n",
            "loss: 0.347503  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.374275 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.214166  [    0/60000]\n",
            "loss: 0.317729  [ 6400/60000]\n",
            "loss: 0.212709  [12800/60000]\n",
            "loss: 0.323725  [19200/60000]\n",
            "loss: 0.277211  [25600/60000]\n",
            "loss: 0.330716  [32000/60000]\n",
            "loss: 0.303328  [38400/60000]\n",
            "loss: 0.419102  [44800/60000]\n",
            "loss: 0.394974  [51200/60000]\n",
            "loss: 0.345915  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.371753 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.212209  [    0/60000]\n",
            "loss: 0.313155  [ 6400/60000]\n",
            "loss: 0.211267  [12800/60000]\n",
            "loss: 0.317825  [19200/60000]\n",
            "loss: 0.275335  [25600/60000]\n",
            "loss: 0.328656  [32000/60000]\n",
            "loss: 0.300333  [38400/60000]\n",
            "loss: 0.411742  [44800/60000]\n",
            "loss: 0.387672  [51200/60000]\n",
            "loss: 0.342525  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.369821 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.211580  [    0/60000]\n",
            "loss: 0.309936  [ 6400/60000]\n",
            "loss: 0.209328  [12800/60000]\n",
            "loss: 0.312958  [19200/60000]\n",
            "loss: 0.273607  [25600/60000]\n",
            "loss: 0.325213  [32000/60000]\n",
            "loss: 0.298331  [38400/60000]\n",
            "loss: 0.405139  [44800/60000]\n",
            "loss: 0.383138  [51200/60000]\n",
            "loss: 0.339425  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.368210 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.210938  [    0/60000]\n",
            "loss: 0.305907  [ 6400/60000]\n",
            "loss: 0.206337  [12800/60000]\n",
            "loss: 0.308108  [19200/60000]\n",
            "loss: 0.272246  [25600/60000]\n",
            "loss: 0.322500  [32000/60000]\n",
            "loss: 0.295648  [38400/60000]\n",
            "loss: 0.398140  [44800/60000]\n",
            "loss: 0.375561  [51200/60000]\n",
            "loss: 0.337425  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.366578 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.210547  [    0/60000]\n",
            "loss: 0.304074  [ 6400/60000]\n",
            "loss: 0.204482  [12800/60000]\n",
            "loss: 0.303052  [19200/60000]\n",
            "loss: 0.270823  [25600/60000]\n",
            "loss: 0.319253  [32000/60000]\n",
            "loss: 0.294680  [38400/60000]\n",
            "loss: 0.391846  [44800/60000]\n",
            "loss: 0.370367  [51200/60000]\n",
            "loss: 0.335668  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.364453 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.210208  [    0/60000]\n",
            "loss: 0.300578  [ 6400/60000]\n",
            "loss: 0.203277  [12800/60000]\n",
            "loss: 0.298976  [19200/60000]\n",
            "loss: 0.269905  [25600/60000]\n",
            "loss: 0.316925  [32000/60000]\n",
            "loss: 0.291618  [38400/60000]\n",
            "loss: 0.385251  [44800/60000]\n",
            "loss: 0.365824  [51200/60000]\n",
            "loss: 0.332937  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.362712 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.209726  [    0/60000]\n",
            "loss: 0.295714  [ 6400/60000]\n",
            "loss: 0.201676  [12800/60000]\n",
            "loss: 0.294278  [19200/60000]\n",
            "loss: 0.268015  [25600/60000]\n",
            "loss: 0.313424  [32000/60000]\n",
            "loss: 0.289738  [38400/60000]\n",
            "loss: 0.380282  [44800/60000]\n",
            "loss: 0.360488  [51200/60000]\n",
            "loss: 0.330232  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.360295 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.207105  [    0/60000]\n",
            "loss: 0.292277  [ 6400/60000]\n",
            "loss: 0.199021  [12800/60000]\n",
            "loss: 0.290316  [19200/60000]\n",
            "loss: 0.268117  [25600/60000]\n",
            "loss: 0.310457  [32000/60000]\n",
            "loss: 0.285349  [38400/60000]\n",
            "loss: 0.375213  [44800/60000]\n",
            "loss: 0.355606  [51200/60000]\n",
            "loss: 0.328083  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.359400 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.208403  [    0/60000]\n",
            "loss: 0.289739  [ 6400/60000]\n",
            "loss: 0.197198  [12800/60000]\n",
            "loss: 0.286530  [19200/60000]\n",
            "loss: 0.269509  [25600/60000]\n",
            "loss: 0.307136  [32000/60000]\n",
            "loss: 0.282201  [38400/60000]\n",
            "loss: 0.369737  [44800/60000]\n",
            "loss: 0.346878  [51200/60000]\n",
            "loss: 0.325260  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.357875 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.208455  [    0/60000]\n",
            "loss: 0.286623  [ 6400/60000]\n",
            "loss: 0.194714  [12800/60000]\n",
            "loss: 0.282418  [19200/60000]\n",
            "loss: 0.266766  [25600/60000]\n",
            "loss: 0.303205  [32000/60000]\n",
            "loss: 0.279269  [38400/60000]\n",
            "loss: 0.364405  [44800/60000]\n",
            "loss: 0.343224  [51200/60000]\n",
            "loss: 0.323111  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.356043 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.206501  [    0/60000]\n",
            "loss: 0.283734  [ 6400/60000]\n",
            "loss: 0.191742  [12800/60000]\n",
            "loss: 0.277795  [19200/60000]\n",
            "loss: 0.267430  [25600/60000]\n",
            "loss: 0.300936  [32000/60000]\n",
            "loss: 0.276558  [38400/60000]\n",
            "loss: 0.358767  [44800/60000]\n",
            "loss: 0.336537  [51200/60000]\n",
            "loss: 0.322123  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.354652 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.205915  [    0/60000]\n",
            "loss: 0.280442  [ 6400/60000]\n",
            "loss: 0.190104  [12800/60000]\n",
            "loss: 0.274047  [19200/60000]\n",
            "loss: 0.265661  [25600/60000]\n",
            "loss: 0.296829  [32000/60000]\n",
            "loss: 0.272785  [38400/60000]\n",
            "loss: 0.354847  [44800/60000]\n",
            "loss: 0.332994  [51200/60000]\n",
            "loss: 0.319464  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.353310 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.205774  [    0/60000]\n",
            "loss: 0.277368  [ 6400/60000]\n",
            "loss: 0.188385  [12800/60000]\n",
            "loss: 0.269203  [19200/60000]\n",
            "loss: 0.264502  [25600/60000]\n",
            "loss: 0.294419  [32000/60000]\n",
            "loss: 0.271498  [38400/60000]\n",
            "loss: 0.347246  [44800/60000]\n",
            "loss: 0.329773  [51200/60000]\n",
            "loss: 0.316205  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.352530 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.205663  [    0/60000]\n",
            "loss: 0.273336  [ 6400/60000]\n",
            "loss: 0.185157  [12800/60000]\n",
            "loss: 0.265100  [19200/60000]\n",
            "loss: 0.262325  [25600/60000]\n",
            "loss: 0.292008  [32000/60000]\n",
            "loss: 0.268587  [38400/60000]\n",
            "loss: 0.342937  [44800/60000]\n",
            "loss: 0.323893  [51200/60000]\n",
            "loss: 0.314650  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.351350 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.206601  [    0/60000]\n",
            "loss: 0.270388  [ 6400/60000]\n",
            "loss: 0.183756  [12800/60000]\n",
            "loss: 0.262282  [19200/60000]\n",
            "loss: 0.260947  [25600/60000]\n",
            "loss: 0.288285  [32000/60000]\n",
            "loss: 0.263868  [38400/60000]\n",
            "loss: 0.337858  [44800/60000]\n",
            "loss: 0.320693  [51200/60000]\n",
            "loss: 0.312844  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.348886 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.202935  [    0/60000]\n",
            "loss: 0.267095  [ 6400/60000]\n",
            "loss: 0.181477  [12800/60000]\n",
            "loss: 0.259387  [19200/60000]\n",
            "loss: 0.259196  [25600/60000]\n",
            "loss: 0.284667  [32000/60000]\n",
            "loss: 0.261456  [38400/60000]\n",
            "loss: 0.331405  [44800/60000]\n",
            "loss: 0.316965  [51200/60000]\n",
            "loss: 0.310878  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.348361 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.204032  [    0/60000]\n",
            "loss: 0.266523  [ 6400/60000]\n",
            "loss: 0.179939  [12800/60000]\n",
            "loss: 0.255473  [19200/60000]\n",
            "loss: 0.258370  [25600/60000]\n",
            "loss: 0.281268  [32000/60000]\n",
            "loss: 0.258973  [38400/60000]\n",
            "loss: 0.325734  [44800/60000]\n",
            "loss: 0.313045  [51200/60000]\n",
            "loss: 0.309285  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.347880 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.203351  [    0/60000]\n",
            "loss: 0.263477  [ 6400/60000]\n",
            "loss: 0.177029  [12800/60000]\n",
            "loss: 0.251449  [19200/60000]\n",
            "loss: 0.257460  [25600/60000]\n",
            "loss: 0.277954  [32000/60000]\n",
            "loss: 0.256517  [38400/60000]\n",
            "loss: 0.321953  [44800/60000]\n",
            "loss: 0.310082  [51200/60000]\n",
            "loss: 0.304549  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.348118 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.203532  [    0/60000]\n",
            "loss: 0.259357  [ 6400/60000]\n",
            "loss: 0.174253  [12800/60000]\n",
            "loss: 0.248965  [19200/60000]\n",
            "loss: 0.258439  [25600/60000]\n",
            "loss: 0.275791  [32000/60000]\n",
            "loss: 0.253249  [38400/60000]\n",
            "loss: 0.316425  [44800/60000]\n",
            "loss: 0.305294  [51200/60000]\n",
            "loss: 0.302067  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.346570 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.203056  [    0/60000]\n",
            "loss: 0.256565  [ 6400/60000]\n",
            "loss: 0.171842  [12800/60000]\n",
            "loss: 0.245806  [19200/60000]\n",
            "loss: 0.256207  [25600/60000]\n",
            "loss: 0.271770  [32000/60000]\n",
            "loss: 0.249461  [38400/60000]\n",
            "loss: 0.313098  [44800/60000]\n",
            "loss: 0.302778  [51200/60000]\n",
            "loss: 0.300217  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.346532 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.203547  [    0/60000]\n",
            "loss: 0.252037  [ 6400/60000]\n",
            "loss: 0.169851  [12800/60000]\n",
            "loss: 0.243616  [19200/60000]\n",
            "loss: 0.255722  [25600/60000]\n",
            "loss: 0.267905  [32000/60000]\n",
            "loss: 0.246089  [38400/60000]\n",
            "loss: 0.309383  [44800/60000]\n",
            "loss: 0.301008  [51200/60000]\n",
            "loss: 0.296070  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.344254 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.200449  [    0/60000]\n",
            "loss: 0.248746  [ 6400/60000]\n",
            "loss: 0.167109  [12800/60000]\n",
            "loss: 0.239990  [19200/60000]\n",
            "loss: 0.254406  [25600/60000]\n",
            "loss: 0.264813  [32000/60000]\n",
            "loss: 0.242806  [38400/60000]\n",
            "loss: 0.302836  [44800/60000]\n",
            "loss: 0.296503  [51200/60000]\n",
            "loss: 0.294618  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.344420 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving the model**"
      ],
      "metadata": {
        "id": "5CmwxzbLVdGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "XnpPfiHmBbDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImGkjtWUBiE6",
        "outputId": "88ec4bb0-792b-4652-af9e-180a90ee11f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proof on an instance**\n"
      ],
      "metadata": {
        "id": "D98DUSoaTVXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_set[0][0], test_set[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDini13yBqQJ",
        "outputId": "2f2cf907-13de-4ca7-de84-012b5a353f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    }
  ]
}